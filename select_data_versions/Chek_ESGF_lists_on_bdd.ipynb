{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# List ESGF latest version for a series of variables and check them on /bdd/CMIP6"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  Define lists of interesting variables and experiments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "parameters"
    ]
   },
   "outputs": [],
   "source": [
    "experiments=[\"piControl\",\"historical\",\"ssp126\",\"ssp245\",\"ssp585\",\"ssp119\"]\n",
    "#experiments=[\"ssp126\"]#,\"ssp245\",\"ssp585\",\"ssp119\"]\n",
    "\n",
    "variables={\n",
    "    \"Amon\": [\"pr\",\"tas\",\"prw\",\"evspsbl\"], \n",
    "    \"Lmon\": [\"mrro\",\"mrso\",\"mrsos\"],\n",
    "    \"Omon\": [\"sos\"],\n",
    "    \"day\" : [\"pr\"]\n",
    "    }\n",
    "\n",
    "CAMMAC             = \"/home/ssenesi/CAMMAC\"\n",
    "\n",
    "node=\"esgf-data.dkrz.de\" \n",
    "\n",
    "do_test=True\n",
    "\n",
    "institutions=dict()\n",
    "activities=dict()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if do_test :\n",
    "    experiments=[\"historical\"]\n",
    "    experiments=[\"ssp245\"]\n",
    "    #experiments=[\"piControl\"]\n",
    "    variables={\"Amon\": [\"pr\"]}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.core.display import display, HTML, Image\n",
    "display(HTML(\"<style>.container { width:100% !important; }</style>\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests  # use pip or conda to install it if needed\n",
    "import json\n",
    "import glob\n",
    "import re\n",
    "import os\n",
    "if not os.path.exists(\"timestamps\"):\n",
    "    os.mkdir(\"timestamps\")\n",
    "from climaf import period\n",
    "sys.path.append(CAMMAC    ) \n",
    "from CAMMAClib.ancillary   import feed_dic, amail"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## A basic function for querying the ESGF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def jrequest(q,node=node) :  \n",
    "    #\"esgf-node.ipsl.upmc.fr\"    #\"esgf-data.dkrz.de\"    #\"esgf-node.jpl.nasa.gov\"\n",
    "    form=\"&format=application%2Fsolr%2Bjson\"\n",
    "    reqs=\"http://%s/esg-search/search?%s%s\"%(node,q,form)\n",
    "    #print reqs\n",
    "    return requests.get(reqs).json()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def published_period(model,experiment,variable,table,variant=None,version=None,no_published_files=[]) :\n",
    "    dic={'distrib'     : 'true',\n",
    "         'limit'       : '10000',\n",
    "         'type'        : 'File',\n",
    "         'fields'      : 'title,instance_id,variant_label,version',\n",
    "         'experiment_id':experiment,\n",
    "         'source_id'   : model,\n",
    "         'variable'    : variable,\n",
    "         'table_id'    : table,\n",
    "         'latest'      : 'true',\n",
    "         'replica'     : 'false',\n",
    "         #'id'     : \"*\"+version+\"*\",\n",
    "         }\n",
    "    #\n",
    "    # Form request string\n",
    "    reqs=\"\"\n",
    "    for k in dic : reqs+=\"%s=%s&\"%(k,dic[k])\n",
    "    reqs=reqs[0:-1]\n",
    "    #\n",
    "    rep=jrequest(reqs)\n",
    "    #return rep\n",
    "    docs=rep['response']['docs']\n",
    "    if len(docs)==0 : \n",
    "        return ([])\n",
    "    #None\n",
    "    #\n",
    "    # Analyze response to gather periods among all files and shards\n",
    "    periods=dict()\n",
    "    for e in docs :\n",
    "        #print \"%-30s %s\"%(e['data_node'],e['title'])\n",
    "        instance_id=e['instance_id'].encode('ascii')\n",
    "        #realization=instance_id.split(\".\")[5]\n",
    "        #version=instance_id.split(\".\")[9]\n",
    "        realization=e['variant_label'][0].encode('ascii')\n",
    "        version=e['version'][0].encode('ascii')\n",
    "        \n",
    "        #\n",
    "        filename=e['title'].encode('ascii')\n",
    "        #if \"2569\" in filename : print \"got one:\",filename,instance_id\n",
    "        file_period=filename.split(\"_\")[-1].replace(\".nc\",\"\")\n",
    "        if (realization,version) not in periods :\n",
    "            periods[(realization,version)]=set()\n",
    "        periods[(realization,version)].add(file_period)\n",
    "    for pair in periods :\n",
    "        ps=list(periods[pair])\n",
    "        ps.sort()\n",
    "        if len(ps) > 900 : ps=ps[0:900] # Issue with some models ...\n",
    "        periods[pair]=period.merge_periods([ period.init_period(p) for p in ps ],handle_360_days_year=True)\n",
    "    if variant is None :\n",
    "        return periods\n",
    "    else :\n",
    "        #if model==\"INM-CM5-0\" :\n",
    "        #    print periods\n",
    "        if (variant,version) in periods :\n",
    "            return periods[(variant,version)]\n",
    "        else:\n",
    "            print(\"no file for dataset %s %s %s %s %s %s \"%(model,experiment,table,variable,variant,version))\n",
    "            print periods.keys()\n",
    "            #no_published_files.append((model,experiment,variable,table,variant,version))\n",
    "            return []\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_period_length(experiment,filenames=[],periods=None):\n",
    "    #\n",
    "    lengths={ \"piControl\" :200, \"historical\":165, \"ssp119\":86, \"ssp126\":86, \"ssp245\":86, \"ssp585\":86 }\n",
    "    if periods is None :\n",
    "        if len(filenames)==0 :\n",
    "            return False\n",
    "        else :\n",
    "            periods=set()\n",
    "            for filename in filenames :\n",
    "                file_period=filename.split(\"_\")[-1].replace(\".nc\",\"\")\n",
    "                periods.add(file_period)\n",
    "            ps=list(periods)\n",
    "            ps.sort()\n",
    "            if len(ps) > 900 : ps=ps[0:900] # Isseu with some models ...\n",
    "            periods=period.merge_periods([ period.init_period(p) for p in ps ],handle_360_days_year=True)\n",
    "    #\n",
    "    try :\n",
    "        aperiod=periods[0]\n",
    "    except :\n",
    "        # print \"Issue with \",periods, \n",
    "        #if filenames != [] : print filenames[0].split(\"/\")[-1]\n",
    "        #else: print\n",
    "        #raise Error(\"\")\n",
    "        return False\n",
    "    length = aperiod.end.year - aperiod.start.year +1\n",
    "    #print \"exp\",experiment,\"length\",length\n",
    "    ok = length >= lengths[experiment]\n",
    "    if not ok :\n",
    "        if len(periods) > 2 : \n",
    "            periods=periods[0:2]+[\"...\"]\n",
    "        if filenames != [] :\n",
    "            fnames = filenames[0].split(\"/\")[-1]\n",
    "        else :\n",
    "            fnames = \"\"\n",
    "        #print \"Not enough data for \",fnames, periods\n",
    "    #else : \n",
    "    #    print \"Fine for \",filenames[0].split(\"/\")[-1]\n",
    "    return ok"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Function for comparing ESGF content and /bdd content for a variable + experiment\n",
    "### It queries the ESGF using replica=false and distrib=true\n",
    "### It returns two lists : one for ESGF datasets which are present on /bdd, and one for those which are missing \n",
    "### The first list is also organized as a multi-level dict with variant and version as last key levels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def list_datasets(experiment,variable,table,distrib=\"true\",limit=10000, already_checked_OK=[],published_NOK=[],published_OK=[]) :\n",
    "    #\n",
    "    ok=[]\n",
    "    nok=[]\n",
    "    dok=dict()\n",
    "    #\n",
    "    # Form ESGF API request string\n",
    "    dic={'distrib'     : distrib,\n",
    "         'limit'       : str(limit),\n",
    "         'type'        : 'Dataset',\n",
    "         'fields'      : 'activity_id,institution_id,source_id,variant_label,grid_label,version,instance_id',\n",
    "         'experiment_id':experiment,\n",
    "         'variable'    : variable,\n",
    "         'table_id'    : table,\n",
    "         'latest'     : 'true',\n",
    "         'replica'     : 'false'\n",
    "         }\n",
    "    reqs=\"\"\n",
    "    for k in dic : reqs+=\"%s=%s&\"%(k,dic[k])\n",
    "    reqs=reqs[0:-1]\n",
    "    #\n",
    "    #print reqs\n",
    "    req=jrequest(reqs)\n",
    "    docs=req['response']['docs']\n",
    "    if len(docs)==0 : \n",
    "        return ok,dok,nok\n",
    "    #\n",
    "    #\n",
    "    count_missing=0\n",
    "    count_ok=0\n",
    "    no_published_files=[]\n",
    "    for e in docs :\n",
    "        model=e['source_id'][0].encode('ascii')\n",
    "        variant=e['variant_label'][0].encode('ascii')\n",
    "        grid=e['grid_label'][0].encode('ascii')\n",
    "        version=e['version'].encode('ascii')\n",
    "        instance_id=e['instance_id']#.encode('ascii')\n",
    "        institution_id=e['institution_id'][0].encode('ascii')\n",
    "        activity_id=e['activity_id'][0].encode('ascii')\n",
    "        nuple=(activity_id,institution_id,model,experiment,variant,table,variable,grid,version)\n",
    "        activities[experiment]=activity_id\n",
    "        institutions[model]=institution_id\n",
    "        #\n",
    "        if list(nuple) not in published_OK :\n",
    "            if list(nuple) not in published_NOK:\n",
    "                pub_period=published_period(model,experiment,variable,table,variant,version,no_published_files)\n",
    "                pub_ok=check_period_length(experiment,periods=pub_period)\n",
    "                if pub_ok :\n",
    "                    published_OK.append(list(nuple))\n",
    "                else :\n",
    "                    published_NOK.append(list(nuple))\n",
    "            else :\n",
    "                pub_ok = False\n",
    "        else: \n",
    "            pub_ok = True\n",
    "        #\n",
    "        if list(nuple) in already_checked_OK :\n",
    "            #print \"already checked:\", nuple\n",
    "            check = \"already_checked\" \n",
    "        else :\n",
    "            if list(nuple) not in published_NOK :\n",
    "                d=\"/bdd/CMIP6/%s/%s/%s/%s/%s/%s/%s/%s/v%s\"%nuple\n",
    "                # Check the \n",
    "                files=glob.glob(d+\"/*nc\")\n",
    "                \n",
    "                # Check if the data directory has already been negatively checked, and \n",
    "                # this later than its last change\n",
    "                timestamp_filename=\"timestamps/%s.%s.%s.%s.%s.%s.%s.%s.v%s\"%nuple\n",
    "                if os.path.exists(timestamp_filename) :\n",
    "                    if not os.path.exists(d) :\n",
    "                        check = \"nok\"\n",
    "                    elif os.path.getmtime(timestamp_filename) < os.path.getmtime(d) :\n",
    "                        # Dir has changed since last check\n",
    "                        os.remove(timestamp_filename)\n",
    "                        check = \"todo_again\"\n",
    "                    else : \n",
    "                        check = \"nok\"\n",
    "                else : \n",
    "                    #if len(files) == 0 : \n",
    "                    #    check = \"nok\"\n",
    "                    #else:\n",
    "                    # Want to create a timestamp file even if data dir does not exist\n",
    "                    check = \"todo\"\n",
    "                #\n",
    "                if \"todo\" in check :\n",
    "                    # Check that the data files do cover the experiment time extent\n",
    "                    #print \"checking files for\",nuple\n",
    "                    print \".\",\n",
    "                    if check_period_length(experiment, filenames=files) :\n",
    "                        check = \"ok\"\n",
    "                    elif pub_ok :\n",
    "                      # Published period si OK while files period is NOK\n",
    "                      check = \"nok\"\n",
    "                      # Create a file which modification time tells the date of this negative check\n",
    "                      with open(timestamp_filename,\"w\") as f : f.write(\"\")\n",
    "                    else :\n",
    "                        check = \"no_use\"\n",
    "            else :\n",
    "                check = \"no_use\"\n",
    "            \n",
    "        if check== \"nok\" :\n",
    "            #check=glob.glob(os.path.dirname(d))\n",
    "            #print \"%10s\"%\"missing \",d, check, instance_id\n",
    "            count_missing+=1\n",
    "            nok.append(nuple)\n",
    "            #if model==\"IPSL-CM6A-LR\" :\n",
    "            #    print d\n",
    "        elif check in [ \"ok\", \"already_checked\" ]:\n",
    "            count_ok+=1\n",
    "            ok.append(nuple)\n",
    "            feed_dic(dok,None,activity_id,institution_id,model,experiment,table,variable,grid,variant,version)\n",
    "            if check != \"already_checked\" :\n",
    "                already_checked_OK.append(list(nuple))\n",
    "            #print \"%10s\"%\"ok \",d\n",
    "        elif check not in  \"no_use\" :\n",
    "            raise ValueError(\"Logic error %s\",check)\n",
    "                \n",
    "    #nopub=dict()\n",
    "    #for e in no_published_files :\n",
    "    #    model,experiment,variable,table,variant,version = e\n",
    "    #    feed_dic(nopub,1,model,use_count=True)\n",
    "    #print \"Datasets without published files\"\n",
    "    #for model in nopub :\n",
    "    #    print model,nopub[model]\n",
    "            \n",
    "    return ok,dok,nok"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Analyze outputs of function above, including counting those datasets which are missing on /bdd and also do not show a sibling variant on /bdd\n",
    "### Print a summary of : \n",
    "- datasets on ESGF which are present on bdd, \n",
    "- datasets on ESGF which are missing on bdd, \n",
    "- missing datasets which have a sibling variant on bdd, \n",
    "- difference of the two latter counts,\n",
    "- number of datsets without a sibling variant whcih have an older version on bdd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def analyze_missings(ok,dok,nok) :    \n",
    "    # Analyze missing datasets which have another variant on /bdd (and/or another version)\n",
    "    \n",
    "    # A dict to list missing variants for each dataset\n",
    "    dnok={}\n",
    "    #\n",
    "    count_other_version = 0\n",
    "    count_other_variant = 0\n",
    "    miss_with_other_version    = []\n",
    "    miss_without_other_version = []\n",
    "    count_missing_single = 0\n",
    "    for nuple in nok :\n",
    "        #\n",
    "        # Check for an older version\n",
    "        activity_id,institution_id,model,experiment,variant,table,variable,grid,version = nuple\n",
    "        d=\"/bdd/CMIP6/%s/%s/%s/%s/%s/%s/%s/%s/v%s/\"%nuple\n",
    "        d=os.path.dirname(os.path.dirname(d))+\"/*\"\n",
    "        #print \"checking\",d\n",
    "        check=glob.glob(d)\n",
    "        if len(check) > 0 :\n",
    "            count_other_version += 1\n",
    "            other_version=True\n",
    "            #print nuple,alt\n",
    "        else : \n",
    "            other_version=False\n",
    "        #\n",
    "        # Check for a variant OK\n",
    "        #\n",
    "        try :\n",
    "            # Get the list of variants\n",
    "            variants=dok[activity_id][institution_id][model][experiment][table][variable][grid].keys()\n",
    "            nb=len(variants)\n",
    "            if nb > 0 : \n",
    "                #print \"for\",nuple, 'found alt variants',alt\n",
    "                count_other_variant += 1\n",
    "            else : \n",
    "                raise ValueError(\"Logic error for \"+str(nuple))\n",
    "        except :\n",
    "            if other_version :\n",
    "                miss_with_other_version.append(nuple)\n",
    "            else :\n",
    "                miss_without_other_version.append(nuple)\n",
    "            # Register this (variant,version) in the list of missing variants for this dataset\n",
    "            feed_dic(dnok,(variant,version),model,experiment,table,variable,grid,use_list=True)\n",
    "\n",
    "    #\n",
    "    count_missing=len(nok)\n",
    "    count_ok=len(ok)\n",
    "    count_with_other_version=len(miss_with_other_version)\n",
    "    count_without_other_version=len(miss_without_other_version)    \n",
    "    #        \n",
    "    if len(nok)==0 :\n",
    "        if len(ok) != 0 :\n",
    "            _,_,_,experiment,_,table,variable,_,_ = ok[0]\n",
    "        else :\n",
    "            experiment,table,variable=\"?\",\"?\",\"?\"\n",
    "    #\n",
    "    print \"%20s\"%experiment, \"%4s\"%table, \"%10s\"%variable, \" OK\",\"%4d\"%count_ok, \"missing\",\"%3d\"%count_missing,\n",
    "    print \"alt_variant\",\"%3d\"%count_other_variant, \"no_variant\",\"%3d\"%(count_missing - count_other_variant),\n",
    "    print \"old_version\",\"%3d\"%count_with_other_version, \"no_version\",\"%3d\"%count_without_other_version, \"%3d\"%count_other_version\n",
    "    \n",
    "    return count_ok,count_missing,count_other_variant,miss_with_other_version,miss_without_other_version,dnok"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "if False :\n",
    "    a=[]\n",
    "    o,d,n=list_datasets(\"historical\",\"pr\",\"day\",limit=200,already_checked_OK=a)\n",
    "    count_ok,count_missing,count_other_variant,mwi,mwo=analyze_missings(o,d,n)\n",
    "    #print mwi\n",
    "    #print mwo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def choose_variant(variants) :\n",
    "    \"\"\" Provided with a list o pairs (variant,version), returns the pair which realization number is the lowest\"\"\"\n",
    "    #\n",
    "    var,ver=variants[0]\n",
    "    rmin=int(re.sub(r\"r([0-9]*)i(.*)\",r\"\\1\",var))\n",
    "    rep=var,ver\n",
    "    #\n",
    "    if len(variants) > 1 :\n",
    "        for var,ver in variants[1:]:\n",
    "            r=int(re.sub(r\"r([0-9]*)i(.*)\",r\"\\1\",var))\n",
    "            if r < rmin :\n",
    "                rep=var,ver\n",
    "    return rep"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  Iterate the analysis of missing datsets over all experiments and variables\n",
    "### And also compute stats of which variant labels are missing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "dic_miss_with    = dict()\n",
    "dic_miss_without = dict()\n",
    "missing_variants = dict()\n",
    "missing_models   = dict()\n",
    "missing_modvar   = dict()\n",
    "missing_modvariables = dict()\n",
    "missing_variables = dict()\n",
    "missing_varmodels = dict()\n",
    "Count_ok            = 0\n",
    "Count_missing       = 0\n",
    "Count_other_variant = 0\n",
    "Count_missing_single= 0\n",
    "\n",
    "try :\n",
    "    with open(\"already_checked.json\",\"r\") as f :\n",
    "        already_checked=json.load(f)\n",
    "except :\n",
    "    already_checked=[]\n",
    "\n",
    "try :\n",
    "    with open(\"published_nok.json\",\"r\") as f :\n",
    "        published_nok=json.load(f)\n",
    "except :\n",
    "    published_nok=[]\n",
    "\n",
    "try :\n",
    "    with open(\"published_ok.json\",\"r\") as f :\n",
    "        published_ok=json.load(f)\n",
    "except :\n",
    "    published_ok=[]\n",
    "\n",
    "\n",
    "with open(\"missing_datasets.txt\",\"w\") as output_file :\n",
    "    for experiment in experiments :\n",
    "        print\n",
    "        for table in variables :\n",
    "            print\n",
    "            for variable in variables[table] :\n",
    "                ok,dok,nok            = list_datasets(experiment,variable,table,\n",
    "                                            already_checked_OK=already_checked,published_NOK=published_nok,published_OK=published_ok)\n",
    "                count_ok,count_missing,count_other_variant,miss_with,miss_without,dnok = analyze_missings(ok,dok,nok)\n",
    "                Count_ok             += count_ok\n",
    "                Count_missing        += count_missing\n",
    "                Count_other_variant  += count_other_variant\n",
    "                feed_dic(dic_miss_with   ,miss_with   ,experiment, table, variable)\n",
    "                feed_dic(dic_miss_without,miss_without,experiment, table, variable)\n",
    "                # Count missings without counting twice if two variants are missing\n",
    "                count_missing_single = 0\n",
    "                for model in dnok :\n",
    "                    for experiment in dnok[model]:\n",
    "                        for table in dnok[model][experiment]:\n",
    "                            for variable in dnok[model][experiment][table]:\n",
    "                                for grid in dnok[model][experiment][table][variable]:\n",
    "                                    count_missing_single +=1\n",
    "                                    variants=dnok[model][experiment][table][variable][grid]\n",
    "                                    Variant,Version=choose_variant(variants)\n",
    "                                    # CMIP6.CMIP.CNRM-CERFACS.CNRM-CM6-1.1pctCO2.r1i1p1f2.Amon.clw.gr.v20180626\n",
    "                                    output_file.write(\"CMIP6.%s.%s.%s.%s.%s.%s.%s.%s.v%s %s\\n\"%\\\n",
    "                                                      (activities[experiment],institutions[model],model,experiment,\\\n",
    "                                                       Variant,table,variable,grid,Version,variants))\n",
    "                                    feed_dic(missing_models  ,1, model  ,         use_count=True)\n",
    "                                    var2=variable\n",
    "                                    if table==\"day\" : var2=var2+\"_day\"\n",
    "                                    feed_dic(missing_modvariables  ,1, model , var2  ,use_count=True)\n",
    "                                    feed_dic(missing_varmodels     ,1, var2  , model ,use_count=True)\n",
    "                                    feed_dic(missing_variables     ,1, var2  ,use_count=True)\n",
    "                Count_missing_single += count_missing_single\n",
    "\n",
    "with open(\"already_checked.json\",\"w\") as f :\n",
    "    json.dump(already_checked,f,separators=(',', ': '),indent=1,ensure_ascii=True)\n",
    "\n",
    "with open(\"published_nok.json\",\"w\") as f :\n",
    "    json.dump(published_nok,f,separators=(',', ': '),indent=1,ensure_ascii=True)\n",
    "\n",
    "with open(\"published_ok.json\",\"w\") as f :\n",
    "    json.dump(published_ok,f,separators=(',', ': '),indent=1,ensure_ascii=True)\n",
    "\n",
    "Count_published_nok=len(published_nok)\n",
    "\n",
    "grand_total    = Count_ok + Count_missing + Count_published_nok\n",
    "total          = Count_ok + Count_missing\n",
    "really_missing = Count_missing - Count_other_variant\n",
    "\n",
    "subject = \"Missings : per variant %3.1f %%, merging variants :%3.1f %% (%d)\"%(100.*Count_missing/total,100.*Count_missing_single/total,Count_missing_single)\n",
    "\n",
    "msg  = \"\"\n",
    "msg +=\"\\nExperiments :%s\"%experiments\n",
    "msg +=\"\\nVariables :%s\"%variables\n",
    "msg += \"\\n\"\n",
    "msg += \"\\n# datasets published : %5d (latest versions)\"%grand_total\n",
    "msg += \"\\n# published OK : %5d (%4.1f %%) (i.e. a long enough period without hole)\"%(total,100.*total/grand_total)\n",
    "msg += \"\\n  # OK on bdd : %5d\"%Count_ok\n",
    "msg += \"\\n  # missings (variant per variant) :  %3d (%3.1f %%)\"%(Count_missing,100.*Count_missing/total)\n",
    "msg += \"\\n  # missings using alt variant : %3d (%3.1f %%)\"%(really_missing,100.*really_missing/total)\n",
    "msg += \"\\n  # same with single counting for multiple missing variants : %3d (%3.1f %%)\"%(Count_missing_single,100.*Count_missing_single/total)\n",
    "msg += \"\\n\"\n",
    "\n",
    "msg += \"\\nMissing datasets per variable (when merging variants): \\n\"\n",
    "for model in sorted(missing_variables.keys()):\n",
    "    msg += \"\\t%-20s : %d %s\\n\"%(model, missing_variables[model], missing_varmodels[model])\n",
    "    \n",
    "msg += \"\\nMissing datasets per model (when merging variants): \\n\"\n",
    "for model in sorted(missing_modvariables.keys()):\n",
    "    msg += \"\\t%-20s : %d %s\\n\"%(model, missing_models[model], missing_modvariables[model])\n",
    "    \n",
    "print msg\n",
    "amail(msg,subject=subject,sender=\"Missing_hydro_datasets_on_bdd\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python",
   "pygments_lexer": "ipython2"
  },
  "kernelspec": {
   "name": "python2",
   "display_name": "Python 2",
   "language": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
